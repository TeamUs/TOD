{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в обработку текста на естественном языке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Материалы:\n",
    "* Макрушин С.В. Лекция 9: Введение в обработку текста на естественном языке\\\n",
    "* https://realpython.com/nltk-nlp-python/\n",
    "* https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\артём\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\артём\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\артём\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\артём\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\артём\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy2 in c:\\users\\артём\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in c:\\users\\артём\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pymorphy2) (0.7.2)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\users\\артём\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pymorphy2) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in c:\\users\\артём\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pymorphy2) (2.4.417127.4579844)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install pymorphy2\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pymorphy2\n",
    "from nltk.metrics.distance import edit_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = \"ПИ19-3\"\n",
    "s2 = \"ПМ19-3\"\n",
    "edit_distance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['высокопревосходительства',\n",
       " 'попреблагорассмотрительст',\n",
       " 'попреблагорассмотрительствующемуся',\n",
       " 'убегающих',\n",
       " 'уменьшившейся']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"велечайшим\"\n",
    "with open (\"litw-win.txt\", \"r\", encoding='windows-1251') as fp:\n",
    "    words = [line.strip().split()[-1] for line in fp]\n",
    "words[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'величайшим'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(words, key=lambda w: edit_distance(w, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Разбейте текст из формулировки задания 1 на слова; проведите стемминг и лемматизацию слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\артём\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\артём\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\артём\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\артём\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\артём\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\артём\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Артём\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "с\n",
      "велечайш\n",
      "усил\n",
      "выбра\n",
      "из\n",
      "поток\n",
      "убега\n",
      "люд\n",
      "кутуз\n",
      "со\n",
      "свит\n",
      "уменьшевш\n",
      "вдво\n",
      "поеха\n",
      "на\n",
      "звук\n",
      "выстрел\n",
      "русск\n",
      "оруд\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('russian')\n",
    "\n",
    "for word in word_tokenize(text):\n",
    "    result = stemmer.stem(word)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "с\n",
      "велечайший\n",
      "усилие\n",
      "выбраться\n",
      "из\n",
      "поток\n",
      "убегать\n",
      "человек\n",
      "кутузов\n",
      "с\n",
      "свита\n",
      "уменьшевшийся\n",
      "вдвое\n",
      "поехать\n",
      "на\n",
      "звук\n",
      "выстрел\n",
      "русский\n",
      "орудие\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "for word in word_tokenize(text):\n",
    "    result = morph.parse(word)[0].normalized.word\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Преобразуйте предложения из формулировки задания 1 в векторы при помощи `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Считайте слова из файла `litw-win.txt` и запишите их в список `words`.',\n",
       " 'В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`.',\n",
       " 'Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`. '''\n",
    "sents = sent_tokenize(text)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'считайте': 32,\n",
       " 'слова': 24,\n",
       " 'из': 12,\n",
       " 'файла': 33,\n",
       " 'litw': 0,\n",
       " 'win': 2,\n",
       " 'txt': 1,\n",
       " 'запишите': 11,\n",
       " 'их': 14,\n",
       " 'список': 31,\n",
       " 'words': 3,\n",
       " 'заданном': 9,\n",
       " 'предложении': 22,\n",
       " 'исправьте': 13,\n",
       " 'все': 5,\n",
       " 'опечатки': 21,\n",
       " 'заменив': 10,\n",
       " 'опечатками': 20,\n",
       " 'на': 16,\n",
       " 'ближайшие': 4,\n",
       " 'смысле': 27,\n",
       " 'расстояния': 23,\n",
       " 'левенштейна': 15,\n",
       " 'ним': 18,\n",
       " 'списка': 29,\n",
       " 'что': 34,\n",
       " 'слове': 25,\n",
       " 'есть': 8,\n",
       " 'опечатка': 19,\n",
       " 'если': 7,\n",
       " 'данное': 6,\n",
       " 'слово': 26,\n",
       " 'не': 17,\n",
       " 'содержится': 28,\n",
       " 'списке': 30}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit(sents)\n",
    "sents_cv = cv.transform(sents).toarray()\n",
    "sents_cv\n",
    "\n",
    "sents_cv.shape\n",
    "\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние редактирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Весего 1069254 слов\n",
      "Среди них 32868 уникальных\n"
     ]
    }
   ],
   "source": [
    "preprocessed_descriptions = pd.read_csv(\"preprocessed_descriptions.csv\")\n",
    "\n",
    "words_set = set()\n",
    "words_list = list()\n",
    "words = [word_tokenize(item) for item in preprocessed_descriptions[\"preprocessed_descriptions\"].to_list() if isinstance(item, str)]\n",
    "\n",
    "[[words_set.add(x) for x in item] for item in words]\n",
    "[[words_list.append(x) for x in item] for item in words]\n",
    "    \n",
    "print(f\"Весего {len(words_list)} слов\\nСреди них {len(words_set)} уникальных\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 orzo uncleour\n",
      "8 stevia worthwhile\n",
      "4 tables famie\n",
      "7 flagrant owing\n",
      "8 meatloaves goodplease\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "data = random.sample(list(words_set), 10)\n",
    "for i in range(0,len(data),2):\n",
    "    x, y = data[i], data[i+1]\n",
    "    print(edit_distance(x, y), x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'seedless'),\n",
       " (1, 'needless'),\n",
       " (2, 'needles'),\n",
       " (2, 'endless'),\n",
       " (3, 'meatless'),\n",
       " (3, 'settlers'),\n",
       " (3, 'restless'),\n",
       " (3, 'shellless'),\n",
       " (3, 'eggless'),\n",
       " (3, 'seekers'),\n",
       " (3, 'sinless')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Set, List\n",
    "\n",
    "def same_words(word: str, k: int, words_data: Set[str]) -> List[str]:\n",
    "    \"\"\"Функция для возврата k подобных слов для word из коллекции words_data\"\"\"\n",
    "    buf_tuple = [(edit_distance(word, item), item) for item in words_data]\n",
    "    buf_tuple.sort(key=lambda x: x[0])\n",
    "    return buf_tuple[:k]\n",
    "\n",
    "same_words(\"seedless\", 11, words_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг, лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами: \n",
    "    * word\n",
    "    * stemmed_word \n",
    "    * normalized_word \n",
    "\n",
    "Столбец `word` укажите в качестве индекса. \n",
    "\n",
    "Для стемминга воспользуйтесь `SnowballStemmer`, для нормализации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Артём\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>stemmed_word</th>\n",
       "      <th>normalized_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>intriguing</td>\n",
       "      <td>intrigu</td>\n",
       "      <td>intrigue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lost</td>\n",
       "      <td>lost</td>\n",
       "      <td>lose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>symbolizes</td>\n",
       "      <td>symbol</td>\n",
       "      <td>symbolize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>recognizing</td>\n",
       "      <td>recogn</td>\n",
       "      <td>recognize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>dredged</td>\n",
       "      <td>dredg</td>\n",
       "      <td>dredge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32748</th>\n",
       "      <td>greased</td>\n",
       "      <td>greas</td>\n",
       "      <td>grease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32757</th>\n",
       "      <td>renaming</td>\n",
       "      <td>renam</td>\n",
       "      <td>rename</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32801</th>\n",
       "      <td>sophisticated</td>\n",
       "      <td>sophist</td>\n",
       "      <td>sophisticate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32834</th>\n",
       "      <td>nabed</td>\n",
       "      <td>nabe</td>\n",
       "      <td>nab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32853</th>\n",
       "      <td>rumbled</td>\n",
       "      <td>rumbl</td>\n",
       "      <td>rumble</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word stemmed_word normalized_word\n",
       "8         intriguing      intrigu        intrigue\n",
       "18              lost         lost            lose\n",
       "21        symbolizes       symbol       symbolize\n",
       "57       recognizing       recogn       recognize\n",
       "65           dredged        dredg          dredge\n",
       "...              ...          ...             ...\n",
       "32748        greased        greas          grease\n",
       "32757       renaming        renam          rename\n",
       "32801  sophisticated      sophist    sophisticate\n",
       "32834          nabed         nabe             nab\n",
       "32853        rumbled        rumbl          rumble\n",
       "\n",
       "[1309 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df = pd.DataFrame(words_set)\n",
    "words_df.columns = ['word']\n",
    "words_df['stemmed_word'] = words_df.apply(lambda x: stemmer.stem(x[\"word\"]), axis=1)\n",
    "words_df['normalized_word'] = words_df.apply(lambda x: lemmatizer.lemmatize(x[\"word\"], \"v\"), axis=1)\n",
    "words_df[(words_df[\"word\"] != words_df[\"normalized_word\"]) & (words_df[\"stemmed_word\"] != words_df[\"normalized_word\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Удалите стоп-слова из описаний рецептов. Какую долю об общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Артём\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего слов: 1069254\n",
      "С удалением стоп-слов: 580889\n",
      "Доля стоп-слов: 54.33%\n"
     ]
    }
   ],
   "source": [
    "words_filtered = [item for item in words_list if item not in stopwords_set]\n",
    "diff = round(len(words_filtered)/len(words_list)*100,2)\n",
    "print(f\"Всего слов: {len(words_list)}\\nС удалением стоп-слов: {len(words_filtered)}\\nДоля стоп-слов: {diff}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### До удаления"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40072 -> the\n",
      "34951 -> a\n",
      "30245 -> and\n",
      "26859 -> this\n",
      "24836 -> i\n",
      "23471 -> to\n",
      "20285 -> is\n",
      "19756 -> it\n",
      "18364 -> of\n",
      "15939 -> for\n"
     ]
    }
   ],
   "source": [
    "freq = nltk.FreqDist(words_list)\n",
    "for word, number in freq.most_common(10):\n",
    "    print(f\"{number} -> {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### После удаления"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14871 -> recipe\n",
      "6326 -> make\n",
      "5137 -> time\n",
      "4620 -> use\n",
      "4430 -> great\n",
      "4167 -> like\n",
      "4152 -> easy\n",
      "3872 -> one\n",
      "3810 -> made\n",
      "3791 -> good\n"
     ]
    }
   ],
   "source": [
    "freq = nltk.FreqDist(words_filtered)\n",
    "for word, number in freq.most_common(10):\n",
    "    print(f\"{number} -> {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторное представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Выберите случайным образом 5 рецептов из набора данных. Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Рецепт 13941:\n",
      "nan\n",
      "Вектор:\n",
      "[[1.]]\n",
      "\n",
      "Рецепт 866:\n",
      "a nice fruity cocktail\n",
      "Вектор:\n",
      "[[0.57735027 0.57735027 0.57735027]]\n",
      "\n",
      "Рецепт 19554:\n",
      "opa  evreryone is greek after a couple of these babies\n",
      "Вектор:\n",
      "[[0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      "  0.33333333 0.33333333 0.33333333]]\n",
      "\n",
      "Рецепт 17637:\n",
      "like my mojito but with amber rum found this on an austrailian sight made a little different then my version depaz amber rum is used in this\n",
      "Вектор:\n",
      "[[0.34299717 0.17149859 0.17149859 0.17149859 0.17149859 0.17149859\n",
      "  0.17149859 0.17149859 0.17149859 0.17149859 0.17149859 0.17149859\n",
      "  0.17149859 0.34299717 0.17149859 0.34299717 0.17149859 0.17149859\n",
      "  0.34299717 0.17149859 0.17149859 0.17149859]]\n",
      "\n",
      "Рецепт 1149:\n",
      "simple simple simpletakes no time to prepare serve with your favorite potato casserole and a some coleslaw\n",
      "Вектор:\n",
      "[[0.23570226 0.23570226 0.23570226 0.23570226 0.23570226 0.23570226\n",
      "  0.23570226 0.23570226 0.47140452 0.23570226 0.23570226 0.23570226\n",
      "  0.23570226 0.23570226 0.23570226]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# загрузка данных из файла\n",
    "data = pd.read_csv('preprocessed_descriptions.csv')\n",
    "data['preprocessed_descriptions']=data['preprocessed_descriptions'].apply(str)\n",
    "# выбор 5 случайных рецептов\n",
    "random_recipes = data.sample(5)\n",
    "\n",
    "# создание объекта TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# преобразование описания каждого рецепта в числовой вектор\n",
    "for i, row in random_recipes.iterrows():\n",
    "    description = row['preprocessed_descriptions']\n",
    "    vector = vectorizer.fit_transform([description])\n",
    "    print(f\"Рецепт {i+1}:\\n{description}\\nВектор:\\n{vector.toarray()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Вычислите близость между каждой парой рецептов, выбранных в задании 3.1, используя косинусное расстояние (`scipy.spatial.distance.cosine`) Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Recipes</th>\n",
       "      <th>13940</th>\n",
       "      <th>865</th>\n",
       "      <th>19553</th>\n",
       "      <th>17636</th>\n",
       "      <th>1148</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recipes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13940</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19553</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.995179</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17636</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.995179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993101</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Recipes  13940  865       19553     17636     1148 \n",
       "Recipes                                            \n",
       "13940      0.0    1.0  1.000000  1.000000  1.000000\n",
       "865        1.0    0.0  1.000000  1.000000  1.000000\n",
       "19553      1.0    1.0  0.000000  0.995179  1.000000\n",
       "17636      1.0    1.0  0.995179  0.000000  0.993101\n",
       "1148       1.0    1.0  1.000000  0.993101  0.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "X = vectorizer.fit_transform(data['preprocessed_descriptions'])\n",
    "\n",
    "# получаем матрицу векторов\n",
    "matrix = X.toarray()\n",
    "\n",
    "# вычисляем близость между каждой парой рецептов\n",
    "distances = cosine_distances(matrix[random_recipes.index])\n",
    "\n",
    "# формируем DataFrame с результатами\n",
    "result_df = pd.DataFrame(distances, index=random_recipes.index, columns=random_recipes.index)\n",
    "result_df.columns.name = 'Recipes'\n",
    "result_df.index.name = 'Recipes'\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Какие рецепты являются наиболее похожими? Прокомментируйте результат (словами)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1074 и 16242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
